{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016-10-07: Regularized Logistic Regression\n",
    "In this lab, we will appply logistic regression to the Endometrium vs. Uterus cancer data.\n",
    "\n",
    "Let us start by setting up our environment, loading the data, and setting up our cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data as usual (here the code for Python 2.7)\n",
    "X = np.loadtxt('data/small_Endometrium_Uterus.csv',  delimiter=',', skiprows=1, usecols=range(1, 3001))\n",
    "y = np.loadtxt('data/small_Endometrium_Uterus.csv', delimiter=',', skiprows=1, usecols=[3001], \n",
    "               converters={3001: lambda s: 0 if s=='Endometrium' else 1}, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a stratified 10-fold cross-validation\n",
    "from sklearn import cross_validation\n",
    "folds = cross_validation.StratifiedKFold(y, 10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function that does cross-validation and scales the features on each training set.\n",
    "from sklearn import preprocessing\n",
    "def cross_validate_with_scaling(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions. \n",
    "    Use a scaler to scale the features to mean 0, standard deviation 1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape) # vector of 0 in which to store the predictions\n",
    "    for tr, te in cv_folds:\n",
    "        # Restrict data to train/test folds\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "        #print Xtr.shape, ytr.shape, Xte.shape\n",
    "\n",
    "        # Scale data\n",
    "        scaler = preprocessing.StandardScaler() # create scaler\n",
    "        Xtr = scaler.fit_transform(Xtr) # fit the scaler to the training data and transform training data\n",
    "        Xte = scaler.transform(Xte) # transform test data\n",
    "\n",
    "        # Fit classifier\n",
    "        classifier.fit(Xtr, ytr)\n",
    "\n",
    "        # Predict probabilities (of belonging to +1 class) on test data\n",
    "        yte_pred = classifier.predict_proba(Xte) # two-dimensional array\n",
    "        # Identify the index, in yte_pred, of the positive class (y=1)\n",
    "        # index_of_class_1 = np.nonzero(classifier.classes_ == 1)[0][0] \n",
    "        index_of_class_1 = 1 - ytr[0] # 0 if the first sample is positive, 1 otherwise\n",
    "        pred[te] = yte_pred[:, index_of_class_1]                \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. L1-Regularized Logistic Regression \n",
    "\n",
    "Let us start with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression(penalty='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compute the cross-validated predictions of the l1-regularized logistic regression with default parameters on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the corresponding ROC curve, and compare it to that obtained for non-regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the C parameter\n",
    "What does the C parameter correspond to? See the documentation at http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression for help.\n",
    "\n",
    "Scikit-learn makes it really easy to use a nested cross-validation to choose a good value for C among a grid of several choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search \n",
    "param_grid = {'C':[1e-3, 1e-2, 1e-1, 1., 1e2, 1e3]}\n",
    "clf = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l1'), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What criterion is used to chose the optimal C? See the documentation at http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV.  Try changing this criterion http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compute the cross-validated predictions of the l1-regularized logistic regression with optimized C parameter on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV also uses the optimal parameter(s) it detected to fit a model to its entire training data again, generating a \"best model\" that is accessible via the `best_estimator_` attribute.\n",
    "\n",
    "In our case, because we called GridSearchCV from inside a cross-validation loop, `clf.best_estimator_` is the \"best model\" *on the last training fold*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the corresponding ROC curve, and compare to that obtained for \n",
    "* non-regularized logistic regression.\n",
    "* l1-regularized logistic regression with default C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression weights\n",
    "Remember the goal of l1-regularization is to build sparse models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code plots the regression weights of the classifier 'clf'\n",
    "plt.plot(range(len(clf.best_estimator_.coef_[0])), clf.best_estimator_.coef_[0], \n",
    "         color='blue', marker='+', linestyle='')\n",
    "plt.xlabel('Genes', fontsize=16)\n",
    "plt.ylabel('Weights', fontsize=16)\n",
    "plt.title('Logistic regression weights', fontsize=16)\n",
    "plt.xlim([0, X.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question** Compare the regression weights obtained with and without l1-regularization, in two side-by-side plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(121) # use a 1x2 subplot grid; ax will refer to the 1st subplot\n",
    "\n",
    "number_of_weights = #TODO\n",
    "logreg_weights = #TODO\n",
    "\n",
    "ax.plot(range(number_of_weights), logreg_weights, \n",
    "         color='blue', marker='+', linestyle='')\n",
    "ax.set_xlabel('Genes', fontsize=16)\n",
    "ax.set_ylabel('Weights', fontsize=16)\n",
    "ax.set_title('Logistic regression weights', fontsize=16)\n",
    "ax.set_xlim([0, X.shape[1]])\n",
    "\n",
    "ax = fig.add_subplot(122) # use a 1x2 subplot grid; ax will refer to the 2nd subplot\n",
    "\n",
    "l1_logreg_weights = #TODO\n",
    "\n",
    "ax.plot(ange(number_of_weights), l1_logreg_weights, \n",
    "         color='blue', marker='+', linestyle='')\n",
    "ax.set_xlabel('Genes', fontsize=16)\n",
    "ax.set_ylabel('Weights', fontsize=16)\n",
    "ax.set_title('Regularized Logistic regression weights', fontsize=16)\n",
    "ax.set_xlim([0, X.shape[1]])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L2-regularized logistic regression\n",
    "\n",
    "**Question** What is the role of l2 regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l2'), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compute the cross-validated predictions of an l2-regularized logistic regression with optimized C parameters on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the corresponding ROC curve, and compare to that obtained for\n",
    "* non-regularized logistic regression\n",
    "* l1-regularized logistic regression (with optimized C parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compare the regression weights obtained with l2-regularization to those obtained \n",
    "* with l1-regularization.\n",
    "* with no regularization.\n",
    "Do your observations match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kaggle challenge\n",
    "* Cross-validate an l1-regularized linear regression (lasso) on your data, using the folds you previously set up for non-regularized linear regression. Do you obtain better performance? Can you draw some conclusions regarding the usefulness of the different features for the prediction task?\n",
    "* Cross-validate an l2-regularized linear regression (ridge regression) on your data, using the folds you previously set up for non-regularized linear regression. Do you obtain better performance?\n",
    "* Submit predictions to the leaderboard for both those models. Do the results on the leaderboard data match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
